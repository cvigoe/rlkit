{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\n",
    "Continuous version by Ian Danforth\n",
    "\"\"\"\n",
    "\n",
    "class ContinuousCartPoleEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 30.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max])\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=self.min_action,\n",
    "            high=self.max_action,\n",
    "            shape=(1,)\n",
    "        )\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def stepPhysics(self, force):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "            (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        return (x, x_dot, theta, theta_dot)\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \\\n",
    "            \"%r (%s) invalid\" % (action, type(action))\n",
    "        # Cast action to float to strip np trappings\n",
    "        force = self.force_mag * float(action)\n",
    "        self.state = self.stepPhysics(force)\n",
    "        self.timer += 1\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        done = x < -self.x_threshold \\\n",
    "            or x > self.x_threshold \\\n",
    "            or theta < -self.theta_threshold_radians \\\n",
    "            or theta > self.theta_threshold_radians \\\n",
    "            or self.timer >= 200\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\"\"\"\n",
    "You are calling 'step()' even though this environment has already returned\n",
    "done = True. You should always call 'reset()' once you receive 'done = True'\n",
    "Any further steps are undefined behavior.\n",
    "                \"\"\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.timer = 0\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width /world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * 1.0\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen-polewidth / 2, -polewidth / 2\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(.8, .6, .4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth / 2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5, .5, .8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=(mode == 'rgb_array'))\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # Shared layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Individual layers\n",
    "        self.output_mean = nn.Linear(32, num_actions)\n",
    "        self.output_std = nn.Linear(32, num_actions)\n",
    "    \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def forward(self, x, network_type):\n",
    "        if network_type == 'mean':\n",
    "            return torch.tanh(self.output_mean(self.layers(torch.from_numpy(np.asarray(x)).float())))\n",
    "        else:\n",
    "            return 0.01 + 100*torch.sigmoid(self.output_std(self.layers(0*torch.from_numpy(np.asarray(x)).float())))\n",
    "\n",
    "    def act(self, state):\n",
    "        action = np.random.multivariate_normal(self.forward(state,'mean').detach().numpy(), np.diag(self.forward(state,'std').detach().numpy()))\n",
    "        return np.clip(action,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(ValueFunction, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(torch.from_numpy(np.asarray(x)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ALGO_TRIALS = 4\n",
    "NUM_ITERATIONS = 600\n",
    "ROLLOUT_LENGTH = 500\n",
    "NUM_TESTS = 1\n",
    "LEARNING_RATE = .00001\n",
    "LEARNING_RATE_V = .0001\n",
    "\n",
    "STATE_DIM = 4\n",
    "ACTION_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbbe345b630>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(17)\n",
    "np.random.seed(17)\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ContinuousCartPoleEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE with (adaptive) Value Function Baseline and Aggregated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE Iteration:  0 \t Evaluation:  10.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  1 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  2 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  3 \t Evaluation:  8.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  4 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  5 \t Evaluation:  5.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  6 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  7 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  8 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  9 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  10 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  11 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  12 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  13 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  14 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  15 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  16 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  17 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  18 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  19 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  20 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  21 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  22 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  23 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  24 \t Evaluation:  5.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  25 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  26 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  27 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  28 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  29 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  30 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  31 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  32 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  33 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  34 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  35 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  36 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  37 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  38 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  39 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  40 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  41 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  42 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  43 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  44 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  45 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  46 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  47 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  48 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  49 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  50 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  51 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  52 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  53 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  54 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  55 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  56 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  57 \t Evaluation:  7.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  58 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  59 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  60 \t Evaluation:  6.00 \tNum Evals:  1\n",
      "REINFORCE Iteration:  61 \t Evaluation:  6.00 \tNum Evals:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-4559fac27bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;31m# Aggregate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     \u001b[0mparam_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Clear sample return if done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/RL_control/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-1d6745e23600>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/RL_control/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/RL_control/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/RL_control/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/RL_control/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/RL_control/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = .0000005\n",
    "LEARNING_RATE_V = .00005\n",
    "\n",
    "# Run the algorithm multiple times to see performance across random seeds\n",
    "performance = np.zeros((NUM_ALGO_TRIALS,NUM_ITERATIONS))\n",
    "\n",
    "for algo_trial in range(NUM_ALGO_TRIALS):\n",
    "    # Initialise policy as a feedforward NN\n",
    "    pi = Policy(STATE_DIM, ACTION_DIM)\n",
    "    v = ValueFunction(STATE_DIM)\n",
    "\n",
    "    # 1 iteration == deploying a fixed policy in the environment for ROLLOUT_LENGTH timesteps and then \n",
    "    # using the collected data from that rollout to perform ROLLOUT_LENGTH policy gradient updates to \n",
    "    # find a new policy for the next iteration\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        # Always reset the state and history for each new policy\n",
    "        state = env.reset()\n",
    "        history = []\n",
    "\n",
    "        # Collect a rollout using the current policy\n",
    "        for timestep in range(ROLLOUT_LENGTH): \n",
    "            action = pi.act(np.asarray(state).flatten())\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            history.append([state, action, reward, next_state, done])\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                \n",
    "        # Sweep backwards through the collected data and use sample_return to accumulate the rewards \n",
    "        # to form the Monte Carlo estimates of the returns\n",
    "        sample_return = 0\n",
    "        \n",
    "        # Reset the gradient aggregators\n",
    "        param_grads = []\n",
    "        torch.log(pi(state,'mean') + pi(state,'std')).backward() # need to backprop something before zero_grad()\n",
    "        pi.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            for index, param in enumerate(pi.parameters()):\n",
    "                param_grads.append(param.grad)\n",
    "        \n",
    "        # Sweep back through collected trajectory and aggregate gradients\n",
    "        for entry in reversed(history):\n",
    "            state, action, reward, next_state, done = entry\n",
    "            sample_return += reward\n",
    "\n",
    "            # Update the Value function baseline\n",
    "            v_loss = (v(state) - sample_return)**2\n",
    "            v.zero_grad()\n",
    "            v_loss.backward()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                for index, param in enumerate(v.parameters()):\n",
    "                    param -= LEARNING_RATE_V * param.grad                                                    \n",
    "\n",
    "            # Clear the gradients in PyTorch computational graph\n",
    "            pi.zero_grad()\n",
    "\n",
    "            # Calculate the gradient of the log-density under the current policy at state, action\n",
    "            p1 = (1/ torch.sqrt((torch.det(torch.diag(pi(state,'std'))))))\n",
    "            M = torch.inverse(torch.diag(pi(state,'std')))\n",
    "            p2 = (torch.from_numpy(action).float() - pi(state,'mean'))\n",
    "            p3 = p2.T @ M @ p2\n",
    "            l = torch.log(p1) + (-0.5*p3)\n",
    "            l.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for index, param in enumerate(pi.parameters()):\n",
    "                    # Aggregate gradients\n",
    "                    param_grads[index] += LEARNING_RATE * param.grad * (reward + v(next_state) - v(state))\n",
    "            \n",
    "            # Clear sample return if done\n",
    "            if done:\n",
    "                sample_return = 0\n",
    "\n",
    "        # After aggregating all the gradients through the log, take a policy gradient step\n",
    "        with torch.no_grad():\n",
    "            for index, param in enumerate(pi.parameters()):\n",
    "                param += param_grads[index]                \n",
    "\n",
    "        # Reset the state so we can perform an independent evaluation rollout of the updated policy\n",
    "        state = env.reset()\n",
    "        evaluation_return = 0\n",
    "        evaluation_returns = []\n",
    "\n",
    "        for test_index in range(NUM_TESTS):\n",
    "            for timestep in range(ROLLOUT_LENGTH):\n",
    "                action = pi.act(np.asarray(state).flatten())            \n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                state = next_state\n",
    "                evaluation_return += reward\n",
    "                if done:\n",
    "                    evaluation_returns.append(evaluation_return)\n",
    "                    evaluation_return = 0\n",
    "                    state = env.reset()\n",
    "                    break\n",
    "        performance[algo_trial][iteration] = np.mean(evaluation_returns)\n",
    "        print('REINFORCE Iteration: ', iteration,'\\t', 'Evaluation: ', \"{:.2f}\".format(performance[algo_trial][iteration]) , '\\tNum Evals: ', len(evaluation_returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4442], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi(state,'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = .00001\n",
    "LEARNING_RATE_V = .0005\n",
    "\n",
    "# Run the algorithm multiple times to see performance across random seeds\n",
    "performance = np.zeros((NUM_ALGO_TRIALS,NUM_ITERATIONS))\n",
    "\n",
    "for algo_trial in range(NUM_ALGO_TRIALS):\n",
    "    # Initialise policy as a feedforward NN\n",
    "    pi = Policy(STATE_DIM, ACTION_DIM)\n",
    "    v = ValueFunction(STATE_DIM)\n",
    "    now = datetime.datetime.now()\n",
    "    writer = SummaryWriter(\"runs/scalar-{}\".format(now.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    # 1 iteration == deploying a fixed policy in the environment for ROLLOUT_LENGTH timesteps and then \n",
    "    # using the collected data from that rollout to perform ROLLOUT_LENGTH policy gradient updates to \n",
    "    # find a new policy for the next iteration\n",
    "    for iteration in range(NUM_ITERATIONS):\n",
    "        # Always reset the state and history for each new policy\n",
    "        state = env.reset()\n",
    "        history = []\n",
    "\n",
    "        for timestep in range(ROLLOUT_LENGTH): \n",
    "            action = pi.act(np.asarray(state).flatten())\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            reward = reward + 0.5*abs(action.item())\n",
    "            history.append([state, action, reward, next_state, done])\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "\n",
    "        # Sweep backwards through the collected data and use sample_return to accumulate the rewards \n",
    "        # to form the Monte Carlo estimates of the returns\n",
    "        sample_return = 0\n",
    "        likelihoods = []\n",
    "        for entry in reversed(history):\n",
    "            state, action, reward, next_state, done = entry\n",
    "            sample_return += reward\n",
    "\n",
    "            # Update the Value function baseline\n",
    "            v_loss = (v(state) - sample_return)**2\n",
    "            v.zero_grad()\n",
    "            v_loss.backward()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                for index, param in enumerate(v.parameters()):\n",
    "                    param -= LEARNING_RATE_V * param.grad                                                    \n",
    "\n",
    "            # Initialise the gradients stored in PyTorch auto-grad computation graph to 0\n",
    "            pi.zero_grad()\n",
    "\n",
    "            # Calculate the gradient of the log-density under the current policy at state, action\n",
    "            p1 = (1/ torch.sqrt((torch.det(torch.diag(pi(state,'std'))))))\n",
    "            M = torch.inverse(torch.diag(pi(state,'std')))\n",
    "            p2 = (torch.from_numpy(action).float() - pi(state,'mean'))\n",
    "            p3 = p2.T @ M @ p2\n",
    "            l = torch.log(p1) + (-0.5*p3)\n",
    "            if l.isnan().any():\n",
    "                raise Exception('About to Blow!')\n",
    "            l.backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                likelihoods.append(l)\n",
    "                grads = []\n",
    "                for index, param in enumerate(pi.parameters()):\n",
    "                    # Perform a gradient ascent step on the policy parameters\n",
    "                    param += LEARNING_RATE * param.grad * (sample_return - v(state))\n",
    "                    grads.extend(param.grad.flatten().tolist())\n",
    "                writer.add_histogram('policy parameters', torch.nn.utils.parameters_to_vector(pi.parameters()), iteration) \n",
    "                writer.add_histogram('policy gradient', np.asarray(grads), iteration)\n",
    "                writer.add_histogram('log-likelihoods', np.asarray(likelihoods), iteration)                \n",
    "                writer.flush()\n",
    "                    \n",
    "            if done:\n",
    "                sample_return = 0\n",
    "\n",
    "                \n",
    "                \n",
    "    param_grads = []\n",
    "    torch.log(pi(state)[action]).backward()\n",
    "    with torch.no_grad():\n",
    "        for index, param in enumerate(pi.parameters()):\n",
    "            param_grads.append(param.grad * 0)\n",
    "\n",
    "#     for entry in reversed(history):\n",
    "#         state, action, reward, next_state, done = entry\n",
    "#         sample_return += reward\n",
    "\n",
    "#         pi.zero_grad()\n",
    "#         torch.log(pi(state)[action]).backward()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for index, param in enumerate(pi.parameters()):\n",
    "                param_grads[index] += LEARNING_RATE * param.grad * sample_return\n",
    "\n",
    "#         if done:\n",
    "#             sample_return = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, param in enumerate(pi.parameters()):\n",
    "            param += param_grads[index]                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        # Reset the state so we can perform an independent evaluation rollout of the updated policy\n",
    "        state = env.reset()\n",
    "        evaluation_return = 0\n",
    "        evaluation_returns = []\n",
    "\n",
    "        for test_index in range(NUM_TESTS):\n",
    "            for timestep in range(ROLLOUT_LENGTH):\n",
    "                action = pi.act(np.asarray(state).flatten())            \n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                state = next_state\n",
    "                evaluation_return += reward\n",
    "                if done:\n",
    "                    evaluation_returns.append(evaluation_return)\n",
    "                    evaluation_return = 0\n",
    "                    state = env.reset()\n",
    "                    break\n",
    "        performance[algo_trial][iteration] = np.mean(evaluation_returns)\n",
    "        print('REINFORCE Iteration: ', iteration,'\\t', 'Evaluation: ', \"{:.2f}\".format(performance[algo_trial][iteration]) , '\\tNum Evals: ', len(evaluation_returns))\n",
    "    writer.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = []\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        state = env.reset()\n",
    "        vals.append(v(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi(state,'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
